---
title: Multinomial Logistic Regression
description: MLR model to classify E, E/M and M in epithelial to mesenchymal transitiojn
authors:
  - name: Markus Owen
format: 
  html:
    embed-resources: true
    code-fold: true
    number-sections: true
    toc: true
    toc-depth: 3
    date: now
    date-modified: last-modified
    date-format: "MMMM DD, YYYY, HH:mm:ss"
jupyter: python3
---

# import packages and define functions

```{python}
#| label: Import_packages

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from numpy.random import default_rng
import math

```

# Test some 3D plots

```{python}
nx = 101
x = np.linspace(0, 2*math.pi, nx)
xv, yv = np.meshgrid(x,x)
ax = plt.figure().add_subplot(projection='3d')

ax.plot_surface(xv,yv,np.sin(xv*yv))

```

# define category probabilities

```{python}
n_cats = 3
alpha_1 = -7.87
alpha_2 = 0.0413
beta_1 = 1.36
beta_2 = -1.96

pi_1 = np.exp(alpha_1-(beta_1*xv+beta_2*yv))/(1.+np.exp(alpha_1-(beta_1*xv+beta_2*yv)))
pi_2 = np.exp(alpha_2-(beta_1*xv+beta_2*yv))/(1.+np.exp(alpha_2-(beta_1*xv+beta_2*yv)))

P1 = pi_1
P2 = pi_2-pi_1
P3 = 1-pi_2

ax = plt.figure().add_subplot(projection='3d')
ax.plot_surface(xv,yv,P1,color='r')
ax.plot_surface(xv,yv,P2,color='g')
ax.plot_surface(xv,yv,P3,color='b')

```

```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
pd.set_option('display.max_rows', 500)

from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, 0:2]  # we only take the first two features for visualization
y = iris.target

n_features = X.shape[1]

random_state=0
seed = 1111

C = 10
kernel = 1.0 * RBF([1.0, 1.0])  # for GPC

# Create different classifiers.
classifiers = {
    'L1 logistic': LogisticRegression(C=C, penalty='l1',
                                      solver='saga',
                                      multi_class='multinomial',
                                      max_iter=10000),
    'L1 logistic (Multinomial)': LogisticRegression(C=C, penalty='l1',
                                                    solver='saga',
                                                    multi_class='multinomial',
                                                    max_iter=10000),
    'L1 logistic (Multinomial again)': LogisticRegression(C=C, penalty='l1',
                                                    solver='saga',
                                                    multi_class='multinomial',
                                                    max_iter=10000),
    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',
                                                    solver='saga',
                                                    multi_class='multinomial',
                                                    max_iter=10000),
    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',
                                            solver='saga',
                                            multi_class='ovr',
                                            max_iter=10000),
    'Linear SVC': SVC(kernel='linear', C=C, probability=True,
                      random_state=0),
    'GPC': GaussianProcessClassifier(kernel)
}

n_classifiers = len(classifiers)

plt.figure(figsize=(3 * 2, n_classifiers * 2))
plt.subplots_adjust(bottom=.2, top=.95)


xx = np.linspace(3, 9, 100)
yy = np.linspace(1, 5, 100).T
xx, yy = np.meshgrid(xx, yy)
Xfull = np.c_[xx.ravel(), yy.ravel()]

for index, (name, classifier) in enumerate(classifiers.items()):
    classifier.fit(X, y)

    y_pred = classifier.predict(X)
    accuracy = accuracy_score(y, y_pred)
    print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))

    # View probabilities:
    probas = classifier.predict_proba(Xfull)
    n_classes = np.unique(y_pred).size
    for k in range(n_classes):
        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)
        plt.title("Class %d" % k)
        if k == 0:
            plt.ylabel(name)
        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),
                                   extent=(3, 9, 1, 5), origin='lower')
        plt.xticks(())
        plt.yticks(())
        idx = (y_pred == k)
        if idx.any():
            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='w', edgecolor='k')

ax = plt.axes([0.15, 0.04, 0.7, 0.05])
plt.title("Probability")
plt.colorbar(imshow_handle, cax=ax, orientation='horizontal')

plt.show()
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression

# make 3-class dataset for classification
centers = [[-5, 0], [0, 1.5], [5, -1]]
X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)
transformation = [[0.4, 0.2], [-0.4, 1.2]]
X = np.dot(X, transformation)

for multi_class in ('multinomial', 'ovr'):
    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,
                             multi_class=multi_class).fit(X, y)

    # print the training scores
    print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))

    # create a mesh to plot in
    h = .02  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
    plt.title("Decision surface of LogisticRegression (%s)" % multi_class)
    plt.axis('tight')

    # Plot also the training points
    colors = "bry"
    for i, color in zip(clf.classes_, colors):
        idx = np.where(y == i)
        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,
                    edgecolor='black', s=20)

    # Plot the three one-against-all classifiers
    xmin, xmax = plt.xlim()
    ymin, ymax = plt.ylim()
    coef = clf.coef_
    intercept = clf.intercept_

    def plot_hyperplane(c, color):
        def line(x0):
            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]
        plt.plot([xmin, xmax], [line(xmin), line(xmax)],
                 ls="--", color=color)

    for i, color in zip(clf.classes_, colors):
        plot_hyperplane(i, color)

plt.show()
```

# Data

https://www.ncbi.nlm.nih.gov/gds?Db=gds&DbFrom=bioproject&Cmd=Link&LinkName=bioproject_gds&LinkReadableName=GEO+DataSets&ordinalpos=1&IdsFromResult=97249

Clicked the [Download Data](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE5846)

Downloaded [GSE5846_family.soft.gz](https://ftp.ncbi.nlm.nih.gov/geo/series/GSE5nnn/GSE5846/soft/GSE5846_family.soft.gz)

It has 1361818 lines. Structure is DATABASE SERIES PLATFORM SAMPLE (repeats for each cell line) sample details sample records starting with sample_table_begin, table with ID_REF (gene?) and VALUE (expression)

Claudin 7 (CLDN7) is REF=202790_at
Vimentin (VIM) is REF=201426_s_at
E-cadherin (CDH1) is REF=201130_s_at

Can get the expression for CLDN7 like this:

```{default}
(base) (base) pmzmro\@MACBOOK-FPN4XWDN41 Downloads % grep 202790_at GSE5846_family.soft \| tail -n +2 \| wc
60 120 1130
```

and

```{default}
(base) pmzmro\@MACBOOK-FPN4XWDN41 Downloads % grep Series_sample_id GSE5846_family.soft \| wc
60 180 1800
```

Just get the values: 
```{default}
(base) pmzmro@MACBOOK-FPN4XWDN41 Downloads % grep 202790_at GSE5846_family.soft | tail -n +2 | cut -w -f2    
5.159424
5.315701
5.595774
5.568423
```

To get just cell line names:
```{default}
(base) pmzmro@MACBOOK-FPN4XWDN41 Downloads % grep Series_sample_id GSE5846_family.soft | cut -w -f3          
GSM136266
GSM136267
GSM136268
GSM136269
```


```{python}
CLDN7 = pd.read_csv('GSE5846_family_CLDN7.txt')
CLDN7.head()

VIM = pd.read_csv('GSE5846_family_VIM.txt')
VIM.head()

CDH1 = pd.read_csv('GSE5846_family_CDH1.txt')
CDH1.head()
```


```{python}
plt.hist(CLDN7)
plt.hist(VIM)
plt.hist(CDH1)
plt.show()
```

[Reference (15)](https://doi.org/10.1101/gad.1640608) apparently gives the E, E/M or M status of these cell lines. 

```{python}
data = pd.concat([CLDN7,VIM,CDH1],axis=1)
data.columns = ['CLDN7', 'VIM', 'CDH1']
data['CDH1/VIM'] = data.CDH1/data.VIM
```