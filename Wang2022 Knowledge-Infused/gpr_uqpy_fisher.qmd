---
jupyter: python3
---


# Gaussian Process with Noise and Constraints

This jupyter script shows the performance of GaussianProcessRegressor class in the UQpy. A training data is generated
using a function ($f(x)$, as defined below), which is used to train a surrogate model.


Import the necessary modules to run the example script. Notice that FminCobyla is used here, to solve the MLE
optimization problem with constraints.


```{python}
#| collapsed: false
import numpy as np
import matplotlib.pyplot as plt
import warnings

from UQpy.surrogates.gaussian_process.regression_models.QuadraticRegression import QuadraticRegression

warnings.filterwarnings('ignore')
from UQpy.utilities.MinimizeOptimizer import MinimizeOptimizer
from UQpy.utilities.FminCobyla import FminCobyla
from UQpy.surrogates import GaussianProcessRegression, NonNegative

from UQpy.utilities.kernels.euclidean_kernels import RBF
```


```{python}
from UQpy.surrogates.gaussian_process.constraints.baseclass.Constraints import ConstraintsGPR

class MyNonNegative(ConstraintsGPR):
    def __init__(self, constraint_points, constraint_model_values, observed_error=0.01, z_value=2, pi_thresh=0.01):
        self.constraint_points = constraint_points
        self.observed_error = observed_error
        self.z_value = z_value
        self.pi_thresh = pi_thresh
        self.constraint_model_values = constraint_model_values
        self.kwargs = {}
        self.constraint_args = None

    def define_arguments(self, x_train, y_train, predict_function):
        self.kwargs['x_t'] = x_train
        self.kwargs['y_t'] = y_train
        self.kwargs['pred'] = predict_function
        self.kwargs['const_points'] = self.constraint_points
        self.kwargs['obs_err'] = self.observed_error
        self.kwargs['z_'] = self.z_value
        self.kwargs['pi_thresh'] = self.pi_thresh
        self.kwargs['const_model_values'] = self.constraint_model_values
        self.constraint_args = [self.kwargs]
        return self.constraints

    @staticmethod
    def constraints(theta_, kwargs):
        x_t, y_t, pred = kwargs['x_t'], kwargs['y_t'], kwargs['pred']
        const_points, obs_err, z_ = kwargs['const_points'], kwargs['obs_err'], kwargs['z_']
        pi_thresh = kwargs['pi_thresh']
        const_model_values = kwargs['const_model_values'] # should be vector of model values at the const_points (different to data values)

        tmp_predict, tmp_error = pred(const_points, True, hyperparameters=10**theta_)
        constraint1 = tmp_predict - z_ * tmp_error

        tmp_predict2 = pred(x_t, False, hyperparameters=10**theta_)
        constraint2 = obs_err - np.abs(y_t[:, 0] - tmp_predict2)

        constraint3 = pi_thresh - np.abs(tmp_predict - const_model_values) 

        constraints = np.concatenate((constraint1, constraint2, 10*constraint3), axis=None)
        return constraints
```


Define the training data set. 

```{python}
# Parameters for Fisher's equation
D = 0.025
r = 0.5
L = 10  # Length of the domain
T = 5   # Total time
dx = 0.1
dt = 0.01

# Discretize the domain
x = np.arange(0, L + dx, dx)
Nx = len(x)
t = np.arange(0, T + dt, dt)
u = np.zeros((len(t), Nx))

# Initial condition: localized population
u[0, :] = np.exp(-0.5 * (x - L/2)**2)

# Finite difference method to solve Fisher's equation
for n in range(0, len(t) - 1):
    u[n + 1, 1:-1] = u[n, 1:-1] + D * dt / dx**2 * (u[n, 2:] - 2 * u[n, 1:-1] + u[n, :-2]) + r * dt * u[n, 1:-1] * (1 - u[n, 1:-1])

# Select labelled data and add noise
label_start = 5
label_sep = 20
label_i = np.arange(label_start,Nx-1,label_sep)
X_train = x[label_i].reshape(-1, 1)
y_train = u[-1, label_i]*(1 + np.random.normal(0, 0.1, size=u[-1, label_i].shape))  # Adding noise

```

Define the test data.


```{python}
#| collapsed: false
# X_test = np.linspace(0, 1, 100).reshape(-1, 1)
X_test = x.reshape(-1, 1)
# y_test = funct(X_test)
```


Plot the datasets

```{python}
fig, ax = plt.subplots(figsize=(8.5,7))
ax.plot(x,u[-1,:],'r--',linewidth=2,label='True solution')
ax.plot(X_train,y_train,'bo',markerfacecolor='b', markersize=10, label='Labelled Training Data')
ax.tick_params(axis='both', which='major', labelsize=12)
ax.set_xlabel('x', fontsize=15)
ax.set_ylabel('f(x)', fontsize=15)
ax.set_ylim([-0.3,1.8])
ax.legend(loc="upper right",prop={'size': 12});
plt.grid()

```

The GP model should be one that maps intensity to tumour cell density, NOT position to density. 


## Train GPR
- Noise
- Constraints

Here, 30 equidistant point are selected over the domain of $x$, lets call them constraint points. The idea is to
train the surrogate model such that the probability of positive surrogates prediction is very high at these points.


```{python}
#| collapsed: false
X_c = np.linspace(0, 1, 31).reshape(-1,1)
X_c = X_test
# X_c = x[np.arange(5,Nx-1,10)].reshape(-1,1)
# y_c = funct(X_c)
```

In this approach, MLE problem is solved with the following constraints:

\begin{align}\hat{y}(x_c)-Z \sigma_{\hat{y}}(x_c) > 0  \quad \quad Z = 2\end{align}
\begin{align}|\hat{y}(x_t) - y(x_t)| < \epsilon   \quad \quad \epsilon = 0.3\end{align}

where, $x_c$ and $x_t$ are the constraint and training sample points, respectively.

Define kernel used to define the covariance matrix. Here, the application of Radial Basis Function (RBF) kernel is
demonstrated.


```{python}
#| collapsed: false
kernel3 = RBF()
```

Define the optimizer used to identify the maximum likelihood estimate.


```{python}
#| collapsed: false
bounds_3 = [[10**(-6), 10**(-1)], [10**(-5), 10**(-1)], [10**(-13), 10**(-5)]]
optimizer3 = FminCobyla()
```

Define constraints for the Cobyla optimizer using UQpy's Nonnegatice class.


```{python}
#| collapsed: false
cons = MyNonNegative(constraint_points=X_c, constraint_model_values=u[-1,], observed_error=0.03, z_value=2)
```

Define the 'GaussianProcessRegressor' class object, the input attributes defined here are kernel, optimizer, initial
estimates of hyperparameters and number of times MLE is identified using random starting point.


```{python}
#| collapsed: false
gpr3 = GaussianProcessRegression(kernel=kernel3, hyperparameters=[10**(-3), 10**(-2), 10**(-10)], optimizer=optimizer3,
                                 optimizations_number=10, optimize_constraints=cons, bounds=bounds_3, noise=True,
                                 regression_model=QuadraticRegression())
```

Call the 'fit' method to train the surrogate model (GPR).


```{python}
#| collapsed: false
gpr3.fit(X_train, y_train)
```

The maximum likelihood estimates of the hyperparameters are as follows:


```{python}
#| collapsed: false
print(gpr3.hyperparameters)

print('Length Scale: ', gpr3.hyperparameters[0])
print('Process Variance: ', gpr3.hyperparameters[1])
print('Noise Variance: ', gpr3.hyperparameters[2])
```

Use 'predict' method to compute surrogate prediction at the test samples. The attribute 'return_std' is a boolean
indicator. If 'True', 'predict' method also returns the standard error at the test samples.


```{python}
#| collapsed: false
y_pred3, y_std3 = gpr3.predict(X_test, return_std=True)
```

The plot shows the test function in dashed red line and 13 training points are represented by blue dots. Also, blue
curve shows the GPR prediction for $x \in (0, 1)$ and yellow shaded region represents 95% confidence interval.


```{python}
#| collapsed: false
fig, ax = plt.subplots(figsize=(8.5,7))
ax.plot(x,u[-1,:],'r--',linewidth=2,label='Test Function')
ax.plot(X_train,y_train,'bo',markerfacecolor='b', markersize=10, label='Training Data')
ax.plot(X_test,y_pred3,'b-', lw=2, label='GP Prediction')
ax.plot(X_test, np.zeros((X_test.shape[0],1)))
ax.fill_between(X_test.flatten(), y_pred3-1.96*y_std3,
                y_pred3+1.96*y_std3,
                facecolor='yellow',label='95% Credibility Interval')
ax.tick_params(axis='both', which='major', labelsize=12)
ax.set_xlabel('x', fontsize=15)
ax.set_ylabel('f(x)', fontsize=15)
ax.set_ylim([-0.3,1.8])
ax.legend(loc="upper right",prop={'size': 12});
plt.grid()
```

Verify the constraints for the trained surrogate model. Notice that all values are positive, thus constraints are
satisfied for the constraint points.


```{python}
#| collapsed: false
y_, ys_ = gpr3.predict(X_c, return_std=True)
y_ - 2*ys_
```

Notice that all values are negative, thus constraints are satisfied for the training points.


```{python}
#| collapsed: false
y_ = gpr3.predict(X_train, return_std=False)
np.abs(y_train-y_) - 0.03
```

```{python}

plt.plot(X_test[1:-1],D*(y_pred3[:-2]-2*y_pred3[1:-1]+y_pred3[2:])/dx**2,'-x')
plt.plot(X_test[1:-1],r*y_pred3[1:-1]*(1-y_pred3[1:-1]),'-x')
plt.title('Approximation to diffusion and growth terms')
plt.show()
```