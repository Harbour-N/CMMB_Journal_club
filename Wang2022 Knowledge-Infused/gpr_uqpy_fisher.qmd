---
jupyter: python3
---


# Gaussian Process with Noise and Constraints

This document is based on an example use of the GaussianProcessRegressor class in UQpy. Training data is generated
using the PI model for tumour growth which gives tumour density which itself is filtered to give a surrogate for an MRI image intensity. Biopsy samples are simulated as specific points for which we know the image intensity (the model input) and the ground truth tumour density (the model output). 

The aim is to include the broader simulation data as a constraint to improve the prediction of tumour density from image intensity.  

Import the necessary modules to run the example script. Notice that FminCobyla is used here, to solve the MLE
optimization problem with constraints.


```{python}
#| collapsed: false
import numpy as np
import matplotlib.pyplot as plt
import warnings

from UQpy.surrogates.gaussian_process.regression_models.QuadraticRegression import QuadraticRegression

warnings.filterwarnings('ignore')
from UQpy.utilities.MinimizeOptimizer import MinimizeOptimizer
from UQpy.utilities.FminCobyla import FminCobyla
from UQpy.surrogates import GaussianProcessRegression, NonNegative

from UQpy.utilities.kernels.euclidean_kernels import RBF
```


```{python}
from UQpy.surrogates.gaussian_process.constraints.baseclass.Constraints import ConstraintsGPR

class MyNonNegative(ConstraintsGPR):
    def __init__(self, constraint_points, constraint_model_values, observed_error=0.01, z_value=2, pi_thresh=0.01):
        self.constraint_points = constraint_points
        self.observed_error = observed_error
        self.z_value = z_value
        self.pi_thresh = pi_thresh
        self.constraint_model_values = constraint_model_values
        self.kwargs = {}
        self.constraint_args = None

    def define_arguments(self, x_train, y_train, predict_function):
        self.kwargs['x_t'] = x_train
        self.kwargs['y_t'] = y_train
        self.kwargs['pred'] = predict_function
        self.kwargs['const_points'] = self.constraint_points
        self.kwargs['obs_err'] = self.observed_error
        self.kwargs['z_'] = self.z_value
        self.kwargs['pi_thresh'] = self.pi_thresh
        self.kwargs['const_model_values'] = self.constraint_model_values
        self.constraint_args = [self.kwargs]
        return self.constraints

    @staticmethod
    def constraints(theta_, kwargs):
        x_t, y_t, pred = kwargs['x_t'], kwargs['y_t'], kwargs['pred']
        const_points, obs_err, z_ = kwargs['const_points'], kwargs['obs_err'], kwargs['z_']
        pi_thresh = kwargs['pi_thresh']
        const_model_values = kwargs['const_model_values'] # should be vector of model values at the const_points (different to data values)

        tmp_predict, tmp_error = pred(const_points, True, hyperparameters=10**theta_)
        constraint1 = tmp_predict - z_ * tmp_error

        tmp_predict2 = pred(x_t, False, hyperparameters=10**theta_)
        constraint2 = obs_err - np.abs(y_t[:, 0] - tmp_predict2)

        constraint3 = pi_thresh - np.abs(tmp_predict - const_model_values) 

        constraints = np.concatenate((constraint1, constraint2, 1*constraint3), axis=None)
        return constraints
```


## Define the training data set. 

Simulate the PI model to give $u(x,t_{final})$. 

We want to produce `simulated_image`, a 1D image intensity derived from $u(x,t_{final})$; `labelled_samples_density`, the ground truth tumour density at a set of sample locations; and `labelled_samples_image_features` which is the image intensity at the same locations. 

```{python}
# Parameters for Fisher's equation
D = 0.025
r = 0.5
L = 10  # Length of the domain
T = 5   # Total time
dx = 0.1
dt = 0.01

# Discretize the domain
x = np.arange(0, L + dx, dx)
Nx = len(x)
t = np.arange(0, T + dt, dt)
u = np.zeros((len(t), Nx))

# Initial condition: localized population
u[0, :] = np.exp(-0.5 * (x - L/2)**2)

# Finite difference method to solve Fisher's equation
for n in range(0, len(t) - 1):
    u[n + 1, 1:-1] = u[n, 1:-1] + D * dt / dx**2 * (u[n, 2:] - 2 * u[n, 1:-1] + u[n, :-2]) + r * dt * u[n, 1:-1] * (1 - u[n, 1:-1])

# Select labelled data and add noise
label_start = 15
label_sep = 15
label_i = np.arange(label_start,Nx-1,label_sep)

simulated_image = 0.2 + 0.8*u[-1,]*(1 + np.random.normal(0, 0.2, size=Nx)) # with noise

x_labelled_samples = x[label_i]
labelled_samples_density = u[-1, label_i] # the "ground truth" tumour cell density at labelled samples
labelled_samples_image_features = simulated_image[label_i]

```

Define the test data. Just the set of space points. 


```{python}
#| collapsed: false
X_test = x.reshape(-1, 1)
```


Plot the datasets

```{python}
fig, ax = plt.subplots(figsize=(8.5,7))
ax.plot(x,u[-1,:],'r--',linewidth=2,label='True solution')
ax.plot(x,simulated_image,'b-',linewidth=1,label='Simulated image')
ax.plot(x_labelled_samples,labelled_samples_density,'ro',markerfacecolor='r', markersize=10, label='Labelled truth')
ax.plot(x_labelled_samples,labelled_samples_image_features,'bo',markerfacecolor='b', markersize=10, label='Labelled Training Data')
ax.tick_params(axis='both', which='major', labelsize=12)
ax.set_xlabel('x', fontsize=15)
ax.set_ylabel('f(x)', fontsize=15)
ax.set_ylim([-0.3,1.8])
ax.legend(loc="upper right",prop={'size': 12});
plt.grid()

```

The GP model should be one that maps intensity to tumour cell density, NOT position to density. 


## Train GPR
- Noise
- Constraints

Here, all spatial mesh points are constraint points. The idea is to
train the surrogate model such that the probability of positive surrogates prediction is very high at these points.


```{python}
#| collapsed: false
X_c = X_test
```

In this approach, MLE problem is solved with the following constraints:

\begin{align}\hat{y}(x_c)-Z \sigma_{\hat{y}}(x_c) > 0  \quad \quad Z = 2\end{align}
\begin{align}|\hat{y}(x_t) - y(x_t)| < \epsilon   \quad \quad \epsilon = 0.3\end{align}

where, $x_c$ and $x_t$ are the constraint and training sample points, respectively.

Define kernel used to define the covariance matrix. Here, the application of Radial Basis Function (RBF) kernel is
demonstrated.


```{python}
#| collapsed: false
kernel3 = RBF()
```

Define the optimizer used to identify the maximum likelihood estimate.


```{python}
#| collapsed: false
bounds_3 = [[10**(-6), 10**(-1)], [10**(-5), 10**(-1)], [10**(-13), 10**(-5)]]
optimizer3 = FminCobyla()
```

Define constraints for the Cobyla optimizer using UQpy's Nonnegatice class.


```{python}
#| collapsed: false
cons = MyNonNegative(constraint_points=X_c, constraint_model_values=u[-1,], observed_error=0.03, z_value=2)
```

Define the 'GaussianProcessRegressor' class object, the input attributes defined here are kernel, optimizer, initial
estimates of hyperparameters and number of times MLE is identified using random starting point.


```{python}
#| collapsed: false
gpr3 = GaussianProcessRegression(kernel=kernel3, hyperparameters=[10**(-3), 10**(-2), 10**(-10)], optimizer=optimizer3,
                                 optimizations_number=10, optimize_constraints=cons, bounds=bounds_3, noise=True,
                                 regression_model=QuadraticRegression())
```

Call the 'fit' method to train the surrogate model (GPR).


```{python}
#| collapsed: false
gpr3.fit(labelled_samples_image_features.reshape(-1, 1), labelled_samples_density.reshape(-1, 1))
```

The maximum likelihood estimates of the hyperparameters are as follows:


```{python}
#| collapsed: false
print(gpr3.hyperparameters)

print('Length Scale: ', gpr3.hyperparameters[0])
print('Process Variance: ', gpr3.hyperparameters[1])
print('Noise Variance: ', gpr3.hyperparameters[2])
```

Use 'predict' method to compute surrogate prediction at the test samples. The attribute 'return_std' is a boolean
indicator. If 'True', 'predict' method also returns the standard error at the test samples.


```{python}
#| collapsed: false
y_pred3, y_std3 = gpr3.predict(simulated_image.reshape(-1, 1), return_std=True)
```

The plot shows the test function in dashed red line and 13 training points are represented by blue dots. Also, blue
curve shows the GPR prediction for $x \in (0, 1)$ and yellow shaded region represents 95% confidence interval.


```{python}
#| collapsed: false
fig, ax = plt.subplots(figsize=(8.5,7))
ax.plot(x,u[-1,:],'r--',linewidth=2,label='Test Function')
ax.plot(x_labelled_samples,labelled_samples_density,'bo',markerfacecolor='b', markersize=10, label='Training Data')
ax.plot(X_test,y_pred3,'b-', lw=2, label='GP Prediction')
ax.plot(X_test, np.zeros((X_test.shape[0],1)))
ax.fill_between(X_test.flatten(), y_pred3-1.96*y_std3,
                y_pred3+1.96*y_std3,
                facecolor='yellow',label='95% Credibility Interval')
ax.tick_params(axis='both', which='major', labelsize=12)
ax.set_xlabel('x', fontsize=15)
ax.set_ylabel('f(x)', fontsize=15)
ax.set_ylim([-0.3,1.8])
ax.legend(loc="upper right",prop={'size': 12});
plt.grid()
```

```{python}
image_max = simulated_image.max()
image_min = simulated_image.min()

T1_range = np.arange(image_min-0.1*(image_max-image_min),image_max+0.1*(image_max-image_min),0.01).reshape(-1, 1)

yT1, yT1_std = gpr3.predict(T1_range, return_std=True)

plt.plot(T1_range,yT1,label='model prediction')
plt.fill_between(T1_range.flatten(), yT1-1.96*yT1_std,yT1+1.96*yT1_std,facecolor='yellow',label='95% Credibility Interval')
plt.plot(labelled_samples_image_features,labelled_samples_density,'+',label='labelled samples')
plt.xlabel('Image intensity')
plt.ylabel('Predicted tumour density')
plt.legend()

```

Verify the constraints for the trained surrogate model. Notice that all values are positive, thus constraints are
satisfied for the constraint points.


```{python}
#| collapsed: false
y_, ys_ = gpr3.predict(X_c, return_std=True)
y_ - 2*ys_
```

Notice that all values are negative, thus constraints are satisfied for the training points.


```{python}
#| collapsed: false
y_ = gpr3.predict(x_labelled_samples.reshape(-1, 1), return_std=False)
np.abs(labelled_samples_density-y_) - 0.03
```

```{python}

plt.plot(X_test[1:-1],D*(y_pred3[:-2]-2*y_pred3[1:-1]+y_pred3[2:])/dx**2,'-x',label='Diffusion')
plt.plot(X_test[1:-1],r*y_pred3[1:-1]*(1-y_pred3[1:-1]),'-x',label='growth')
plt.title('Approximation to diffusion and growth terms')
plt.legend()
plt.show()
```